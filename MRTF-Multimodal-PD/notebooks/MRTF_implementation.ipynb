{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0287cee1",
   "metadata": {},
   "source": [
    "\n",
    "# پیاده‌سازی گام‌به‌گام MRTF (بر اساس متن روش پیشنهادی)\n",
    "\n",
    "این نوت‌بوک یک پیاده‌سازی **قابل‌اجرا (Reference Implementation)** از چارچوب پیشنهادی **MRTF** برای تشخیص زودهنگام پارکینسون است.  \n",
    "در این پیاده‌سازی، بخش‌های زیر مطابق دیاگرام و متن روش پیشنهادی ساخته می‌شوند:\n",
    "\n",
    "1. **پیش‌پردازش سه مدالیته**: صدا، MRI، و سنسور پوشیدنی  \n",
    "2. **استخراج ویژگی مدالیته‌محور**:\n",
    "   - Voice Encoder: **CNN + BiLSTM**\n",
    "   - MRI Encoder: **Vision Transformer (ViT)**\n",
    "   - Sensor Encoder: **TCN + Temporal Transformer**\n",
    "3. **همجوشی چندمدالیته** با **Cross-Attention Fusion Transformer (CAFT)** (سه‌جهته)\n",
    "4. **لایه توضیح‌پذیری**: SHAP (تقریبی) + کانترفکتوال (gradient-based)\n",
    "5. **RASL** (Reinforcement-Assisted Self-Learning): به‌روزرسانی سیاست/آستانه/وزن مدالیته‌ها با سیگنال پاداش\n",
    "6. **آموزش مشترک**: زیان کل `L_total = L_CE + λ1 L_RL + λ2 L_reg`\n",
    "\n",
    "> نکته مهم: چون در این محیط دیتاست واقعی در اختیار نیست، یک دیتاست شبیه‌سازی‌شده (Dummy Dataset) ساخته می‌شود تا کل pipeline از ابتدا تا انتها قابل اجرا باشد.  \n",
    "> کافیست بعداً جای Dummy Dataset را با دیتاست واقعی خودتان (Voice/MRI/Sensor) جایگزین کنید.\n",
    "\n",
    "---\n",
    "\n",
    "## پیش‌نیازها\n",
    "- PyTorch\n",
    "- torchvision (برای ViT)\n",
    "- numpy\n",
    "- (اختیاری) shap — اگر نصب نبود، ما fallback ساده داریم.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1243950",
   "metadata": {},
   "source": [
    "## 1) ایمپورت کتابخانه‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    import torchvision\n",
    "    from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "    _has_torchvision = True\n",
    "except Exception as e:\n",
    "    _has_torchvision = False\n",
    "    print(\"torchvision/ViT not available -> fallback to simple CNN encoder for MRI. Error:\", e)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457bec32",
   "metadata": {},
   "source": [
    "## 2) تنظیمات کلی و هایپرپارامترها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600bb468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Embedding dim (طبق متن: d=512)\n",
    "    d: int = 512\n",
    "\n",
    "    # Voice spectrogram size (dummy)\n",
    "    voice_freq_bins: int = 128\n",
    "    voice_time_steps: int = 256\n",
    "\n",
    "    # MRI image size (طبق متن: 224x224)\n",
    "    mri_size: int = 224\n",
    "\n",
    "    # Sensor window (dummy)\n",
    "    sensor_channels: int = 6   # accel(3)+gyro(3)\n",
    "    sensor_len: int = 200      # ~ 5s * 40Hz (مثال)\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 8\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    epochs: int = 3  # برای دمو کم گذاشته شده\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # Loss weights\n",
    "    lambda_rl: float = 0.5\n",
    "    lambda_reg: float = 1e-4\n",
    "\n",
    "    # RL reward weights (Eq.20)\n",
    "    alpha1_acc: float = 1.0\n",
    "    alpha2_ecs: float = 0.5\n",
    "    alpha3_err: float = 1.0\n",
    "\n",
    "    gamma: float = 0.95  # discount\n",
    "\n",
    "cfg = Config()\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa85963a",
   "metadata": {},
   "source": [
    "## 3) دیتاست نمونه (Dummy) — برای اجرای کامل Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb00bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DummyParkinsonMultimodalDataset(Dataset):\n",
    "    \"\"\" \n",
    "    دیتاست شبیه‌سازی‌شده:\n",
    "    - Voice: spectrogram tensor [1, F, T]\n",
    "    - MRI: image tensor [3, 224, 224]\n",
    "    - Sensor: timeseries [C, L]\n",
    "    - y: label {0,1}\n",
    "    \"\"\"\n",
    "    def __init__(self, n=200, cfg: Config = cfg, seed=42):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.n = n\n",
    "        self.cfg = cfg\n",
    "        self.y = rng.integers(0, 2, size=n).astype(np.int64)\n",
    "\n",
    "        # برای اینکه داده کمی \"قابل یادگیری\" باشد، یک بایاس کوچک روی کلاس 1 ایجاد می‌کنیم\n",
    "        self.voice = rng.normal(0, 1, size=(n, 1, cfg.voice_freq_bins, cfg.voice_time_steps)).astype(np.float32)\n",
    "        self.mri = rng.normal(0, 1, size=(n, 3, cfg.mri_size, cfg.mri_size)).astype(np.float32)\n",
    "        self.sensor = rng.normal(0, 1, size=(n, cfg.sensor_channels, cfg.sensor_len)).astype(np.float32)\n",
    "\n",
    "        pos_idx = self.y == 1\n",
    "        self.voice[pos_idx] += 0.15\n",
    "        self.mri[pos_idx] += 0.05\n",
    "        self.sensor[pos_idx] += 0.10\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"voice\": torch.from_numpy(self.voice[idx]),\n",
    "            \"mri\": torch.from_numpy(self.mri[idx]),\n",
    "            \"sensor\": torch.from_numpy(self.sensor[idx]),\n",
    "            \"y\": torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "ds = DummyParkinsonMultimodalDataset(n=120, cfg=cfg)\n",
    "dl = DataLoader(ds, batch_size=cfg.batch_size, shuffle=True)\n",
    "batch = next(iter(dl))\n",
    "{k: v.shape for k,v in batch.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b72aff",
   "metadata": {},
   "source": [
    "## 4) ماژول‌های پیش‌پردازش (اسکلت قابل جایگزینی با پیاده‌سازی واقعی)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# در این نوت‌بوک، پیش‌پردازش واقعی (MFCC/STFT, skull stripping, wavelet denoise, ...) را\n",
    "# به صورت اسکلت/Placeholder می‌گذاریم چون به دیتای خام و کتابخانه‌های تخصصی نیاز دارد.\n",
    "# شما کافیست در پروژه‌تان این توابع را با پیاده‌سازی واقعی جایگزین کنید.\n",
    "\n",
    "def preprocess_voice(voice_wave_or_spec: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"فرض می‌کنیم ورودی spectrogram است. در عمل: STFT/MFCC/نرمال‌سازی.\"\"\"\n",
    "    x = voice_wave_or_spec\n",
    "    return (x - x.mean(dim=(-1,-2), keepdim=True)) / (x.std(dim=(-1,-2), keepdim=True) + 1e-6)\n",
    "\n",
    "def preprocess_mri(mri_img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"در عمل: skull stripping, bias correction, resize, intensity norm.\"\"\"\n",
    "    x = mri_img\n",
    "    return (x - x.mean(dim=(-1,-2), keepdim=True)) / (x.std(dim=(-1,-2), keepdim=True) + 1e-6)\n",
    "\n",
    "def preprocess_sensor(sensor_ts: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"در عمل: windowing, wavelet denoise, FFT features, normalization.\"\"\"\n",
    "    x = sensor_ts\n",
    "    return (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc4f079",
   "metadata": {},
   "source": [
    "## 5) Voice Encoder: CNN + BiLSTM (Section 3.2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VoiceCNNBiLSTM(nn.Module):\n",
    "    def __init__(self, d=512, voice_freq_bins=128):\n",
    "        super().__init__()\n",
    "        self.voice_freq_bins = voice_freq_bins\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.proj = nn.Linear(128 * (voice_freq_bins//4), 256)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=256,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.out = nn.Linear(512, d)  # 256*2 -> d\n",
    "\n",
    "    def forward(self, x):  # x: [B,1,F,T]\n",
    "        x = self.cnn(x)     # [B,128,F/4,T/4]\n",
    "        B, C, F, T = x.shape\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()  # [B,T,C,F]\n",
    "        x = x.view(B, T, C*F)                   # [B,T,128*(F)]\n",
    "        x = self.proj(x)                        # [B,T,256]\n",
    "        out, _ = self.lstm(x)                   # [B,T,512]\n",
    "        h_last = out[:, -1, :]                  # [B,512]\n",
    "        emb = self.out(h_last)                  # [B,d]\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132cbc15",
   "metadata": {},
   "source": [
    "## 6) MRI Encoder: ViT (Section 3.2.2) + fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79519418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MRIEncoderViT(nn.Module):\n",
    "    def __init__(self, d=512):\n",
    "        super().__init__()\n",
    "        if _has_torchvision:\n",
    "            weights = ViT_B_16_Weights.DEFAULT\n",
    "            self.vit = vit_b_16(weights=weights)\n",
    "            in_features = self.vit.heads.head.in_features\n",
    "            self.vit.heads = nn.Identity()\n",
    "            self.proj = nn.Linear(in_features, d)\n",
    "        else:\n",
    "            self.vit = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1,1))\n",
    "            )\n",
    "            self.proj = nn.Linear(128, d)\n",
    "\n",
    "    def forward(self, x):  # [B,3,224,224]\n",
    "        if _has_torchvision:\n",
    "            feats = self.vit(x)\n",
    "            return self.proj(feats)\n",
    "        else:\n",
    "            feats = self.vit(x).flatten(1)\n",
    "            return self.proj(feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958371f",
   "metadata": {},
   "source": [
    "## 7) Sensor Encoder: TCN + Temporal Transformer (Section 3.2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel=3, dilation=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        pad = (kernel - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=kernel, dilation=dilation, padding=pad)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.res = nn.Conv1d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x):  # [B,C,L]\n",
    "        y = self.conv(x)\n",
    "        y = y[..., :x.shape[-1]]\n",
    "        y = F.relu(y)\n",
    "        y = self.dropout(y)\n",
    "        return y + self.res(x)\n",
    "\n",
    "class SensorTemporalEncoder(nn.Module):\n",
    "    def __init__(self, d=512, sensor_len=200, sensor_channels=6, n_heads=8, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.tcn = nn.Sequential(\n",
    "            TCNBlock(sensor_channels, 64, dilation=1),\n",
    "            TCNBlock(64, 128, dilation=2),\n",
    "            TCNBlock(128, 256, dilation=4),\n",
    "        )\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, sensor_len, 256) * 0.01)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=256, nhead=n_heads, dim_feedforward=512, batch_first=True\n",
    "        )\n",
    "        self.tr = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.out = nn.Linear(256, d)\n",
    "\n",
    "    def forward(self, x):  # [B,C,L]\n",
    "        z = self.tcn(x)            # [B,256,L]\n",
    "        z = z.permute(0,2,1)       # [B,L,256]\n",
    "        z = z + self.pos_emb[:, :z.shape[1], :]\n",
    "        z = self.tr(z)             # [B,L,256]\n",
    "        z = z.permute(0,2,1)       # [B,256,L]\n",
    "        pooled = self.pool(z).squeeze(-1)  # [B,256]\n",
    "        return self.out(pooled)            # [B,d]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678dd9d",
   "metadata": {},
   "source": [
    "## 8) CAFT: Cross-Attention Fusion Transformer (Section 3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf779d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d=512, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=d, num_heads=n_heads, dropout=dropout, batch_first=True)\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "        self.ff = nn.Sequential(nn.Linear(d, 2*d), nn.ReLU(), nn.Dropout(dropout), nn.Linear(2*d, d))\n",
    "        self.ln2 = nn.LayerNorm(d)\n",
    "\n",
    "    def forward(self, q, k, v): \n",
    "        attn_out, attn_w = self.mha(q, k, v, need_weights=True, average_attn_weights=True)\n",
    "        x = self.ln(q + attn_out)\n",
    "        x2 = self.ln2(x + self.ff(x))\n",
    "        return x2, attn_w\n",
    "\n",
    "class CAFT(nn.Module):\n",
    "    def __init__(self, d=512, n_heads=8, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.cross_vm = CrossAttention(d, n_heads, dropout)\n",
    "        self.cross_vs = CrossAttention(d, n_heads, dropout)\n",
    "        self.cross_mv = CrossAttention(d, n_heads, dropout)\n",
    "        self.cross_ms = CrossAttention(d, n_heads, dropout)\n",
    "        self.cross_sv = CrossAttention(d, n_heads, dropout)\n",
    "        self.cross_sm = CrossAttention(d, n_heads, dropout)\n",
    "\n",
    "        self.omega = nn.Parameter(torch.ones(6))\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d, nhead=n_heads, dim_feedforward=4*d, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.tr = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "\n",
    "    def forward(self, Ev, Em, Es):\n",
    "        Ev1 = Ev.unsqueeze(1); Em1 = Em.unsqueeze(1); Es1 = Es.unsqueeze(1)\n",
    "\n",
    "        v_m, w_vm = self.cross_vm(Ev1, Em1, Em1)\n",
    "        v_s, w_vs = self.cross_vs(Ev1, Es1, Es1)\n",
    "\n",
    "        m_v, w_mv = self.cross_mv(Em1, Ev1, Ev1)\n",
    "        m_s, w_ms = self.cross_ms(Em1, Es1, Es1)\n",
    "\n",
    "        s_v, w_sv = self.cross_sv(Es1, Ev1, Ev1)\n",
    "        s_m, w_sm = self.cross_sm(Es1, Em1, Em1)\n",
    "\n",
    "        ome = F.softmax(self.omega, dim=0)\n",
    "\n",
    "        Ev_hat = ome[0]*v_m + ome[1]*v_s\n",
    "        Em_hat = ome[2]*m_v + ome[3]*m_s\n",
    "        Es_hat = ome[4]*s_v + ome[5]*s_m\n",
    "\n",
    "        tokens = torch.cat([Ev_hat, Em_hat, Es_hat], dim=1)  # [B,3,d]\n",
    "        H = self.tr(tokens)\n",
    "        H = self.ln(H)\n",
    "\n",
    "        fused = H.mean(dim=1)  # [B,d]\n",
    "        attn_info = {\n",
    "            \"w_vm\": w_vm.detach(), \"w_vs\": w_vs.detach(),\n",
    "            \"w_mv\": w_mv.detach(), \"w_ms\": w_ms.detach(),\n",
    "            \"w_sv\": w_sv.detach(), \"w_sm\": w_sm.detach(),\n",
    "            \"omega\": ome.detach()\n",
    "        }\n",
    "        return fused, attn_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1281774",
   "metadata": {},
   "source": [
    "## 9) Classifier + Fusion Regularization (Eq.16-17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccfe235",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MRTFClassifierHead(nn.Module):\n",
    "    def __init__(self, d=512, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, d//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d//2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, H):\n",
    "        return torch.sigmoid(self.net(H)).squeeze(-1)\n",
    "\n",
    "def fusion_regularization(Ev, Em, Es):\n",
    "    return (Ev-Em).pow(2).mean() + (Ev-Es).pow(2).mean() + (Em-Es).pow(2).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f719b5",
   "metadata": {},
   "source": [
    "## 10) Explainability Layer: SHAP (تقریبی) + Counterfactual + ECS (Section 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_shap_like_attribution(model_fn, x: torch.Tensor, baseline: Optional[torch.Tensor]=None):\n",
    "    \"\"\"تقریب سبک از SHAP با Integrated Gradients روی فضای فیوژن.\"\"\"\n",
    "    x = x.detach()\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(x)\n",
    "    steps = 16\n",
    "    alphas = torch.linspace(0, 1, steps, device=x.device).view(steps, 1, 1)\n",
    "    x_interp = baseline.unsqueeze(0) + alphas * (x.unsqueeze(0) - baseline.unsqueeze(0))  # [steps,B,d]\n",
    "    x_interp.requires_grad_(True)\n",
    "\n",
    "    y = model_fn(x_interp.view(-1, x.shape[-1]))  # [steps*B]\n",
    "    y_sum = y.sum()\n",
    "    grads = torch.autograd.grad(y_sum, x_interp, retain_graph=False, create_graph=False)[0]  # [steps,B,d]\n",
    "    avg_grads = grads.mean(dim=0)  # [B,d]\n",
    "    phi = (x - baseline) * avg_grads\n",
    "    return phi.detach()\n",
    "\n",
    "def find_counterfactual(model_fn, x: torch.Tensor, target_flip: float=0.5, steps=50, lr=1e-1, l2_weight=0.01):\n",
    "    x0 = x.detach()\n",
    "    x_cf = x0.clone().detach().requires_grad_(True)\n",
    "    opt = torch.optim.SGD([x_cf], lr=lr, momentum=0.0)\n",
    "    with torch.enable_grad():\n",
    "        for _ in range(steps):\n",
    "            y = model_fn(x_cf)\n",
    "            loss_flip = (y - target_flip).pow(2).mean()\n",
    "            loss_l2 = l2_weight * (x_cf - x0).pow(2).mean()\n",
    "            loss = loss_flip + loss_l2\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    return x_cf.detach()\n",
    "\n",
    "def compute_ecs(phi: torch.Tensor, tau: float=0.01, clinical_mask: Optional[torch.Tensor]=None):\n",
    "    if clinical_mask is None:\n",
    "        clinical_mask = torch.ones_like(phi)\n",
    "    important = (phi.abs() > tau).float()\n",
    "    ecs = (important * clinical_mask).mean(dim=-1)\n",
    "    return ecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a801a",
   "metadata": {},
   "source": [
    "## 11) RASL (نسخه عملیاتی ساده)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2473e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RASLModule(nn.Module):\n",
    "    def __init__(self, init_threshold=0.5, beta=0.9):\n",
    "        super().__init__()\n",
    "        self.threshold_logit = nn.Parameter(torch.tensor([math.log(init_threshold/(1-init_threshold))], dtype=torch.float32))\n",
    "        self.register_buffer(\"reward_baseline\", torch.tensor(0.0))\n",
    "        self.beta = beta\n",
    "\n",
    "    def threshold(self):\n",
    "        return torch.sigmoid(self.threshold_logit).item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_baseline(self, r: torch.Tensor):\n",
    "        r_mean = r.mean().detach()\n",
    "        self.reward_baseline = self.beta * self.reward_baseline + (1 - self.beta) * r_mean\n",
    "\n",
    "    def rl_loss(self, rewards: torch.Tensor):\n",
    "        adv = rewards - self.reward_baseline\n",
    "        return -(adv.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590c55af",
   "metadata": {},
   "source": [
    "## 12) مدل کامل MRTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d34320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MRTFModel(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.voice_enc = VoiceCNNBiLSTM(d=cfg.d, voice_freq_bins=cfg.voice_freq_bins)\n",
    "        self.mri_enc = MRIEncoderViT(d=cfg.d)\n",
    "        self.sensor_enc = SensorTemporalEncoder(d=cfg.d, sensor_len=cfg.sensor_len, sensor_channels=cfg.sensor_channels)\n",
    "        self.caft = CAFT(d=cfg.d)\n",
    "        self.clf = MRTFClassifierHead(d=cfg.d, dropout=0.3)\n",
    "        self.rasl = RASLModule(init_threshold=0.5)\n",
    "\n",
    "        self.ln_v = nn.LayerNorm(cfg.d)\n",
    "        self.ln_m = nn.LayerNorm(cfg.d)\n",
    "        self.ln_s = nn.LayerNorm(cfg.d)\n",
    "\n",
    "    def forward(self, voice, mri, sensor):\n",
    "        voice = preprocess_voice(voice)\n",
    "        mri = preprocess_mri(mri)\n",
    "        sensor = preprocess_sensor(sensor)\n",
    "\n",
    "        Ev = self.ln_v(self.voice_enc(voice))\n",
    "        Em = self.ln_m(self.mri_enc(mri))\n",
    "        Es = self.ln_s(self.sensor_enc(sensor))\n",
    "\n",
    "        H, attn = self.caft(Ev, Em, Es)\n",
    "        yhat = self.clf(H)\n",
    "        return yhat, (Ev, Em, Es, H, attn)\n",
    "\n",
    "model = MRTFModel(cfg).to(device)\n",
    "sum(p.numel() for p in model.parameters())/1e6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64991df4",
   "metadata": {},
   "source": [
    "## 13) آموزش مشترک (L_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42130f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_cross_entropy(yhat, y):\n",
    "    eps = 1e-7\n",
    "    yhat = torch.clamp(yhat, eps, 1-eps)\n",
    "    return -(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)).mean()\n",
    "\n",
    "def model_fn_on_fused(model: MRTFModel, H_flat: torch.Tensor):\n",
    "    return torch.sigmoid(model.clf.net(H_flat).squeeze(-1))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "def train_one_epoch(model, dl):\n",
    "    model.train()\n",
    "    total = {\"loss\":0.0, \"ce\":0.0, \"fuse\":0.0, \"rl\":0.0, \"reward\":0.0, \"acc\":0.0, \"ecs\":0.0}\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in dl:\n",
    "        voice = batch[\"voice\"].to(device)\n",
    "        mri = batch[\"mri\"].to(device)\n",
    "        sensor = batch[\"sensor\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "\n",
    "        yhat, (Ev, Em, Es, H, attn) = model(voice, mri, sensor)\n",
    "\n",
    "        # XAI\n",
    "        H_detached = H.detach().requires_grad_(True)\n",
    "        phi = compute_shap_like_attribution(lambda z: model_fn_on_fused(model, z), H_detached)\n",
    "        ecs = compute_ecs(phi, tau=0.01)\n",
    "\n",
    "        # (اختیاری) کانترفکتوال سبک برای 1 نمونه\n",
    "        _ = find_counterfactual(lambda z: model_fn_on_fused(model, z), H_detached[:1], steps=10, lr=0.2)\n",
    "\n",
    "        # reward\n",
    "        thr = model.rasl.threshold()\n",
    "        pred = (yhat >= thr).float()\n",
    "        acc = (pred == y).float().mean()\n",
    "        err = 1.0 - acc\n",
    "        R = cfg.alpha1_acc*acc + cfg.alpha2_ecs*ecs.mean() - cfg.alpha3_err*err\n",
    "        model.rasl.update_baseline(R.detach())\n",
    "\n",
    "        # losses\n",
    "        L_CE = binary_cross_entropy(yhat, y)\n",
    "        L_fuse = fusion_regularization(Ev, Em, Es)\n",
    "        L_RL = model.rasl.rl_loss(R.view(1))\n",
    "        loss = L_CE + 0.1*L_fuse + cfg.lambda_rl*L_RL\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total[\"loss\"] += loss.item()\n",
    "        total[\"ce\"] += L_CE.item()\n",
    "        total[\"fuse\"] += L_fuse.item()\n",
    "        total[\"rl\"] += L_RL.item()\n",
    "        total[\"reward\"] += float(R.detach().cpu())\n",
    "        total[\"acc\"] += float(acc.detach().cpu())\n",
    "        total[\"ecs\"] += float(ecs.mean().detach().cpu())\n",
    "        n_batches += 1\n",
    "\n",
    "    for k in total:\n",
    "        total[k] /= max(1, n_batches)\n",
    "    return total\n",
    "\n",
    "# ساخت دیتالودر دمو\n",
    "class DummyParkinsonMultimodalDataset(Dataset):\n",
    "    def __init__(self, n=200, cfg: Config = cfg, seed=42):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.n = n\n",
    "        self.cfg = cfg\n",
    "        self.y = rng.integers(0, 2, size=n).astype(np.int64)\n",
    "        self.voice = rng.normal(0, 1, size=(n, 1, cfg.voice_freq_bins, cfg.voice_time_steps)).astype(np.float32)\n",
    "        self.mri = rng.normal(0, 1, size=(n, 3, cfg.mri_size, cfg.mri_size)).astype(np.float32)\n",
    "        self.sensor = rng.normal(0, 1, size=(n, cfg.sensor_channels, cfg.sensor_len)).astype(np.float32)\n",
    "        pos_idx = self.y == 1\n",
    "        self.voice[pos_idx] += 0.15\n",
    "        self.mri[pos_idx] += 0.05\n",
    "        self.sensor[pos_idx] += 0.10\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"voice\": torch.from_numpy(self.voice[idx]),\n",
    "            \"mri\": torch.from_numpy(self.mri[idx]),\n",
    "            \"sensor\": torch.from_numpy(self.sensor[idx]),\n",
    "            \"y\": torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "ds = DummyParkinsonMultimodalDataset(n=120, cfg=cfg)\n",
    "dl = DataLoader(ds, batch_size=cfg.batch_size, shuffle=True)\n",
    "\n",
    "for ep in range(cfg.epochs):\n",
    "    stats = train_one_epoch(model, dl)\n",
    "    print(f\"epoch={ep+1} | loss={stats['loss']:.4f} | CE={stats['ce']:.4f} | fuse={stats['fuse']:.4f} | RL={stats['rl']:.4f} | R={stats['reward']:.4f} | acc={stats['acc']:.4f} | ECS={stats['ecs']:.4f} | thr={model.rasl.threshold():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4a9f0",
   "metadata": {},
   "source": [
    "## 14) اینفرنس + گزارش توضیح‌پذیری برای یک نمونه"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b4a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "batch = next(iter(dl))\n",
    "voice = batch[\"voice\"].to(device)\n",
    "mri = batch[\"mri\"].to(device)\n",
    "sensor = batch[\"sensor\"].to(device)\n",
    "y = batch[\"y\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    yhat, (Ev, Em, Es, H, attn) = model(voice, mri, sensor)\n",
    "\n",
    "thr = model.rasl.threshold()\n",
    "pred = (yhat >= thr).float()\n",
    "\n",
    "H2 = H[:2].detach().requires_grad_(True)\n",
    "phi2 = compute_shap_like_attribution(lambda z: model_fn_on_fused(model, z), H2)\n",
    "ecs2 = compute_ecs(phi2, tau=0.01)\n",
    "cf2 = find_counterfactual(lambda z: model_fn_on_fused(model, z), H2, steps=20, lr=0.2)\n",
    "\n",
    "print(\"yhat[:5] =\", yhat[:5].detach().cpu().numpy())\n",
    "print(\"pred[:5] =\", pred[:5].detach().cpu().numpy(), \" (thr=\", thr, \")\")\n",
    "print(\"true[:5] =\", y[:5].detach().cpu().numpy())\n",
    "print(\"ECS(first2) =\", ecs2.detach().cpu().numpy())\n",
    "\n",
    "abs_phi = phi2[0].abs().detach().cpu().numpy()\n",
    "top_idx = abs_phi.argsort()[-10:][::-1]\n",
    "print(\"Top-10 fused feature indices:\", top_idx.tolist())\n",
    "print(\"Top-10 attribution values:\", phi2[0][top_idx].detach().cpu().numpy())\n",
    "\n",
    "dist = (cf2 - H2).pow(2).sum(dim=-1).sqrt().detach().cpu().numpy()\n",
    "print(\"Counterfactual L2 distance (first2):\", dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70a498",
   "metadata": {},
   "source": [
    "\n",
    "## 15) جایگزینی با دیتای واقعی\n",
    "\n",
    "### Voice\n",
    "- ورودی مدل در این نوت‌بوک `spectrogram` با شکل `[B,1,F,T]` است.\n",
    "- در پیاده‌سازی واقعی مطابق متن روش:\n",
    "  - پیش‌تأکید (Eq.1)\n",
    "  - STFT (Eq.2) و/یا MFCC\n",
    "  - نرمال‌سازی\n",
    "\n",
    "### MRI\n",
    "- ورودی `[B,3,224,224]` است.\n",
    "- اگر MRI تک‌کاناله است، یا 3 بار تکرار کنید یا encoder را برای 1 کانال تغییر دهید.\n",
    "- skull stripping / bias correction / MNI normalization را پیش از ورود به مدل انجام دهید (Eq.3).\n",
    "\n",
    "### Sensor\n",
    "- ورودی `[B,C,L]` است (acc+gyro).\n",
    "- windowing 5 ثانیه، wavelet denoise، FFT و نرمال‌سازی (Eq.4).\n",
    "\n",
    "### نکته درباره SHAP واقعی\n",
    "در این نوت‌بوک برای سبک بودن، از **Integrated Gradients** به عنوان تقریب attribution استفاده شد.\n",
    "در پروژه واقعی می‌توانید SHAP یا Captum را اضافه کنید و ECS را مطابق Eq.22 محاسبه کنید.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  },
  "title": "MRTF: Multimodal Reinforcement-Assisted Transformer Framework (Implementation)"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
